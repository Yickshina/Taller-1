{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90fd4fef-7721-4bcf-b915-08f117314c6d",
   "metadata": {},
   "source": [
    "Implementación Base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "9b5aa6c5-003b-4913-aead-a437a13c1fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class FCLayer(Layer):\n",
    "    def __init__(self, input_size, output_size, lambda_reg=0):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        weights_error += self.lambda_reg * self.weights\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error\n",
    "\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error\n",
    "\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        if input_data.ndim == 1:\n",
    "            input_data = np.array([[x] for x in input_data])\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "        for i in range(samples):\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "        return result\n",
    "\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        if x_train[0].ndim == 1:\n",
    "            x_train = np.array([[x] for x in x_train])\n",
    "        samples = len(x_train)\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "                err += self.loss(y_train[j], output)\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "            err /= samples\n",
    "            err = np.mean(err)\n",
    "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    sig = sigmoid(x)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_prime(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def leaky_relu_prime(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "def mse(y_true, y_hat):\n",
    "    return (y_true - y_hat)**2\n",
    "\n",
    "def mse_prime(y_true, y_hat):\n",
    "    return 2 * (y_hat - y_true)\n",
    "\n",
    "def bce(y_true, y_hat):\n",
    "    epsilon = 1e-15\n",
    "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "    return -(y_true * np.log(y_hat) + (1 - y_true) * np.log(1 - y_hat))\n",
    "\n",
    "def bce_prime(y_true, y_hat):\n",
    "    epsilon = 1e-15\n",
    "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "    return -(y_true / y_hat) + (1 - y_true) / (1 - y_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46f2a3-e4ed-47c9-8a38-bc0ef7a3138a",
   "metadata": {},
   "source": [
    "Capa de Convolución "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "1a472530-d0d2-4fc4-a944-e8efe3683fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# La capa de convolución hereda de la clase base, madre de todas las capas\n",
    "class ConvLayer(Layer):\n",
    "    def __init__(self, input_shape, kernel_shape, num_kernels, lambda_reg=0):\n",
    "        self.input_shape = input_shape  # Tamaño de la entrada (alto, ancho, profundidad)\n",
    "        self.input_depth = input_shape[2]  # Profundidad de la entrada (número de canales)\n",
    "        self.kernel_shape = kernel_shape  # Tamaño del kernel (alto, ancho)\n",
    "        self.num_kernels = num_kernels  # Número de kernels (cada uno genera una salida)\n",
    "        self.lambda_reg = lambda_reg  # Factor de regularización L2\n",
    "        # La salida tendrá el mismo tamaño de entrada\n",
    "        # esto no tendría por qué ser así, pero nos ahorra problemas para manejar tamaños entre capas\n",
    "        self.output_shape = (input_shape[0], input_shape[1], num_kernels) \n",
    "        # Inicializamos los pesos de forma aleatoria con rango entre -0.5 y 0.5\n",
    "        self.weights = np.random.rand(kernel_shape[0], kernel_shape[1], self.input_depth, num_kernels) - 0.5\n",
    "        self.bias = np.random.rand(num_kernels) - 0.5\n",
    "\n",
    "    # Función para agregar padding a la entrada\n",
    "    # es necesario para mantener el tamaño de la matriz\n",
    "    def pad_input(self, input_matrix, pad):\n",
    "        # Agregamos padding de ceros alrededor de la matriz de entrada\n",
    "        return np.pad(input_matrix, ((pad, pad), (pad, pad)), mode='constant', constant_values=0)\n",
    "\n",
    "    # Correlación 2D\n",
    "    # la parte previa a la convolución, que además sirve de convolución en fordward prop\n",
    "    def correlate(self, input_matrix, kernel):\n",
    "        # Obtenemos las dimensiones de entrada y del kernel\n",
    "        input_height, input_width = input_matrix.shape\n",
    "        kernel_height, kernel_width = kernel.shape\n",
    "        # Calculamos las dimensiones de la salida tras la correlación\n",
    "        output_height = input_height - kernel_height + 1\n",
    "        output_width = input_width - kernel_width + 1\n",
    "        # Inicializamos la salida como una matriz de ceros\n",
    "        output = np.zeros((output_height, output_width))\n",
    "\n",
    "        # Recorremos la entrada y aplicamos la correlación\n",
    "        for i in range(output_height):\n",
    "            for j in range(output_width):\n",
    "                # Las dos iteraciones controlan el movimiento de una ventana que recorre\n",
    "                # a lo ancho y a lo alto la matriz original, extayendo submatrices\n",
    "                sub_matrix = input_matrix[i:i+kernel_height, j:j+kernel_width]  # Seleccionamos la submatriz de la entrada\n",
    "                output[i, j] = np.sum(sub_matrix * kernel)  # Producto punto entre la submatriz y el kernel\n",
    "\n",
    "        return output\n",
    "\n",
    "    # Convolución 2D (correlación con kernel volteado)\n",
    "    # usaremos la convolución en la etapa de backward prop\n",
    "    def convolve(self, input_matrix, kernel):\n",
    "        # Volteamos el kernel en ambas direcciones (filas y columnas)\n",
    "        flipped_kernel = np.flipud(np.fliplr(kernel))\n",
    "        # Aplicamos la correlación con el kernel volteado\n",
    "        return self.correlate(input_matrix, flipped_kernel)\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        # Guardamos la entrada para usarla en backward_propagation\n",
    "        self.input = input_data\n",
    "        # Inicializamos la salida con ceros\n",
    "        self.output = np.zeros(self.output_shape)\n",
    "\n",
    "        # Calculamos el padding en función del tamaño del kernel\n",
    "        pad_h = self.kernel_shape[0] // 2\n",
    "        pad_w = self.kernel_shape[1] // 2\n",
    "\n",
    "        # Recorremos cada filtro en la profundidad de la capa\n",
    "        for k in range(self.num_kernels):\n",
    "            for d in range(self.input_depth):\n",
    "                # Agregamos padding a la entrada actual (canal d)\n",
    "                padded_input = self.pad_input(self.input[:,:,d], pad_h)\n",
    "                # Realizamos la correlación y obtenemos la salida parcial\n",
    "                correlated_output = self.correlate(padded_input, self.weights[:,:,d,k])\n",
    "                # Sumamos el resultado de la correlación al acumulador de la salida\n",
    "                self.output[:,:,k] += correlated_output[:self.output_shape[0], :self.output_shape[1]]  \n",
    "            # Añadimos el bias al filtro actual\n",
    "            self.output[:,:,k] += self.bias[k]\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        # Inicializamos los errores de entrada y de pesos como matrices de ceros\n",
    "        in_error = np.zeros(self.input_shape)\n",
    "        weights_error = np.zeros((self.kernel_shape[0], self.kernel_shape[1], self.input_depth, self.num_kernels))\n",
    "        dBias = np.zeros(self.num_kernels)\n",
    "\n",
    "        # Calculamos el padding basado en el tamaño del kernel\n",
    "        pad_h = self.kernel_shape[0] // 2\n",
    "        pad_w = self.kernel_shape[1] // 2\n",
    "\n",
    "        # Con cada kernel recorreremos cada capa de la entrada. \n",
    "        for k in range(self.num_kernels):\n",
    "            for d in range(self.input_depth):\n",
    "                # Aplicamos padding al error de salida antes de convolucionarlo con los pesos\n",
    "                padded_output_error = self.pad_input(output_error[:,:,k], pad_h)\n",
    "                # Realizamos la convolución inversa y la sumamos al error de entrada\n",
    "                convolved_error = self.convolve(padded_output_error, self.weights[:,:,d,k])\n",
    "                in_error[:,:,d] += convolved_error[:self.input_shape[0], :self.input_shape[1]]  \n",
    "                # Calculamos el error de los pesos usando correlación entre entrada y error\n",
    "                weights_error[:,:,d,k] = self.correlate(self.input[:,:,d], output_error[:,:,k])\n",
    "            # Sumamos el error del bias en la salida actual\n",
    "            dBias[k] = np.sum(output_error[:,:,k])  \n",
    "\n",
    "        # Aplicamos la regularización L2 sobre los errores de los pesos\n",
    "        weights_error += self.lambda_reg * self.weights\n",
    "\n",
    "        # Actualizamos los pesos y los sesgos usando el gradiente descendente\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * dBias\n",
    "\n",
    "        # Retornamos el error de la entrada para la siguiente capa\n",
    "        return in_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "3993a745-3e68-4007-9c51-8b759add5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer(Layer):\n",
    "    \n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        # Aplanamos la entrada\n",
    "        self.output = input_data.flatten().reshape((1,-1))\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        # Remodelamos el error al tamaño original de la entrada\n",
    "        return output_error.reshape(self.input.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1447dc3d-011f-4f1a-aeb3-d40108cc6b7a",
   "metadata": {},
   "source": [
    "Aplicación de Mini Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "707baaba-8137-44a3-b6a6-d54cac3ab357",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatchNetwork(Network):\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate, batch_size):\n",
    "        if x_train[0].ndim == 1:\n",
    "            x_train = np.array([[x] for x in x_train])\n",
    "        \n",
    "        samples = len(x_train)\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            indices = np.arange(samples)\n",
    "            np.random.shuffle(indices)\n",
    "            x_train = x_train[indices]\n",
    "            y_train = y_train[indices]\n",
    "            \n",
    "            for start in range(0, samples, batch_size):\n",
    "                end = min(start + batch_size, samples)\n",
    "                x_batch = x_train[start:end]\n",
    "                y_batch = y_train[start:end]\n",
    "                \n",
    "                batch_err = 0\n",
    "                \n",
    "                for j in range(len(x_batch)):\n",
    "                    output = x_batch[j]\n",
    "                    for layer in self.layers:\n",
    "                        output = layer.forward_propagation(output)\n",
    "                    \n",
    "                    batch_err += self.loss(y_batch[j], output)\n",
    "                    error = self.loss_prime(y_batch[j], output)\n",
    "                    \n",
    "                    for layer in reversed(self.layers):\n",
    "                        error = layer.backward_propagation(error, learning_rate)\n",
    "    \n",
    "                batch_err /= len(x_batch)\n",
    "                err += batch_err\n",
    "    \n",
    "            err /= (samples / batch_size)\n",
    "            err = np.mean(err)\n",
    "          \n",
    "\n",
    "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc99268-ddf4-4637-8e0e-f1e8f5402e83",
   "metadata": {},
   "source": [
    "Prueba con MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "08e4049f-cd6c-4dbe-97ec-ac71c30a9612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# No necesitamos tantos datos.\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X, y = zip(*random.sample(list(zip(X_train, y_train)), 2000))\n",
    "\n",
    "# Sí necesitamos que la forma de X sea la de un vector, en lugar de una matriz. \n",
    "X, y = np.array(X, dtype='float64'), np.array(y, dtype='float64')\n",
    "X = np.reshape(X, (X.shape[0], -1))\n",
    "\n",
    "# Normalizamos Min-Max\n",
    "X= MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# Dividimos la muestra en dos, una para entrenar y otra para testing, como tenemos \n",
    "# muestra de sobra nos damos el lujo de testear con la misma cantidad que entrenamos.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=123)\n",
    "\n",
    "# Necesitamos que y_train sea un valor categórico, en lugar de un dígito entero.\n",
    "y_train_value = y_train # Guardaremos y_train como valor para un observación más abajo.\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c972f03-9220-438e-b5f2-77329fe02998",
   "metadata": {},
   "source": [
    "Red densa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "05b53684-1796-4c76-8bdc-850d62281c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/3   error=0.003153\n",
      "epoch 2/3   error=0.001664\n",
      "epoch 3/3   error=0.001190\n",
      "MATRIZ DE CONFUSIÓN para modelo ANN\n",
      "[[90  0  1  1  0  1  3  1  0  1]\n",
      " [ 0 94  4  4  1  2  2  4  0  2]\n",
      " [ 3  6 37 18  5  1 25  4  4  6]\n",
      " [ 7  5  4 68  2  0  2  5  0  3]\n",
      " [ 5  3  1  4 56  2  9  7  4 10]\n",
      " [10  4  9 30 10  1 11  4  7 11]\n",
      " [ 8  3  5  5  4  0 68  1  1  3]\n",
      " [ 3  3  4  1  8  0  1 67  1  3]\n",
      " [ 6 10 12 15 15  2  9  5 10 13]\n",
      " [ 3  1  5  5 34  0  3 24  3 22]] \n",
      "\n",
      "La exactitud de testeo del modelo ANN es: 0.806\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "start_time = time.time()\n",
    "\n",
    "# Necesitamos identificar cuántos nodos tiene nuestra entrada, y eso depende del tamaño de X.\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "# Crear instancia de MiniBatchNetwork\n",
    "modelMiniBatch = MiniBatchNetwork()\n",
    "\n",
    "# Agregamos capas al modelo\n",
    "modelMiniBatch.add(FCLayer(entrada_dim, 128))\n",
    "modelMiniBatch.add(ActivationLayer(relu, relu_prime))\n",
    "modelMiniBatch.add(FCLayer(128, 64))\n",
    "modelMiniBatch.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "modelMiniBatch.add(FCLayer(64, 10)) \n",
    "modelMiniBatch.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# Asignamos función de pérdida\n",
    "modelMiniBatch.use(bce, bce_prime)\n",
    "\n",
    "# Entrenamos el modelo con datos de entrenamiento\n",
    "modelMiniBatch.fit(X_train, y_train, epochs=3, learning_rate=0.1, batch_size=16)\n",
    "\n",
    "# Usamos el modelo para predecir sobre los datos de prueba (validación)\n",
    "y_hat = modelMiniBatch.predict(X_test)\n",
    "\n",
    "# Transformamos la salida en un vector one-hot encoded\n",
    "# Asumimos que y_hat tiene una forma de (num_samples, 10) donde cada fila es la salida de 10 nodos\n",
    "y_hat = np.array([np.argmax(output) for output in y_hat])  # Obtenemos el índice del nodo con mayor activación\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    " \n",
    "print(f\"Tiempo total de ejecución: {execution_time:.2f} segundos\") \n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf, '\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test, y_hat)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6fa3d1-a480-4466-9cac-40a029af4954",
   "metadata": {},
   "source": [
    "Prueba con Convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "6d461309-5aaa-484b-b3e2-4e43e150a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tabulate\n",
    "\n",
    "# No necesitamos tantos datos.\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X, y = zip(*random.sample(list(zip(X_train, y_train)), 2000))\n",
    "\n",
    "# Sí necesitamos que la forma de X sea la de un vector, en lugar de una matriz. \n",
    "X, y = np.array(X, dtype='float64'), np.array(y, dtype='float64')\n",
    "#X = np.reshape(X, (X.shape[0], -1)) # YA NO QUEREMOS TRANSFORMAR EN VECTO X\n",
    "# AHORA NOS SERVIRÁ QUE SEA UNA MATRIZ, LA PROCESAREMOS CON CAPA DE CONVOLUCIÓN.\n",
    "\n",
    "# APLICAREMOS UN RESHAPE Y NORMALIZACIÓN MÁS AD-HOC PARA ESTE CASO\n",
    "X = X.reshape(X.shape[0], 28, 28, 1)\n",
    "\n",
    "# Dividimos la muestra en dos, una para entrenar y otra para testing, como tenemos \n",
    "# muestra de sobra nos damos el lujo de testear con la misma cantidad que entrenamos.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=123)\n",
    "\n",
    "# Necesitamos que y_train sea un valor categórico, en lugar de un dígito entero.\n",
    "y_train_value = y_train # Guardaremos y_train como valor para un observación más abajo.\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "\n",
    "# ESTANDARIZACION MANUAL: Habíamos usado MinMax, pero\n",
    "# no nos servirá al usar red de convolución, que necesita recibir una grilla.\n",
    "# Además, preferimos usar estandarización (prob normal) en lugar de simple escalado\n",
    "# Calculamos la media y la desviación estándar\n",
    "mean = np.mean(X_train)\n",
    "std = np.std(X_train)\n",
    "# Estandarizamos calculando el estadístico Z, que reemplaza a los datos originales.\n",
    "X_train = (X_train - mean) / std\n",
    "# La misma transformación aplica a los datos de testeo.\n",
    "# Notese que la media y desviación son los que calculamos para la\n",
    "# muestra de entrenamiento. O sea, los datos de prueba son observaciones\n",
    "# limpias, que no siquiera influyeron en la normalización de los datos. \n",
    "X_test = (X_test - mean) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "b7e02b4c-322e-4511-8440-21b7e0ba53f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/10   error=0.003332\n",
      "epoch 2/10   error=0.002481\n",
      "epoch 3/10   error=0.002026\n",
      "epoch 4/10   error=0.001671\n",
      "epoch 5/10   error=0.001562\n",
      "epoch 6/10   error=0.001320\n",
      "epoch 7/10   error=0.001225\n",
      "epoch 8/10   error=0.001111\n",
      "epoch 9/10   error=0.001023\n",
      "epoch 10/10   error=0.000961\n",
      "MATRIZ DE CONFUSIÓN para modelo ANN\n",
      "[[76  0  1  1  0  6  2  0  3  0]\n",
      " [ 0 95  5  1  0  0  1  1  0  0]\n",
      " [ 3  3 80  7  1  0  3  1  1  1]\n",
      " [ 0  1  7 69  0 15  1  3  4  5]\n",
      " [ 1  2  3  0 49  0 10  2  3 25]\n",
      " [ 0  5 12  8  0 66  4  4  3  1]\n",
      " [ 3  2  2  0  1  3 96  0  0  0]\n",
      " [ 1  4  2  0  0  0  0 84  1  5]\n",
      " [ 0  5  9 13  0  7  2  0 68  7]\n",
      " [ 0  2  3  1  3  0  0  4  5 72]] \n",
      "\n",
      "La exactitud de testeo del modelo ANN es: 0.755\n",
      "Se demoró 12.97 minutos.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#Esto se va a demorar, registremos cuanto. \n",
    "inicio = time.time()\n",
    "\n",
    "# Crear instancia de Network\n",
    "model = MiniBatchNetwork()\n",
    "\n",
    "# Cantidad y Tamaño del kernel\n",
    "kernels = 8\n",
    "kernel_size = 3\n",
    "\n",
    "# Tamaño / forma de la entrada\n",
    "input_shape = X_train[0].shape\n",
    "\n",
    "# Parámetro regularizados\n",
    "lambda_reg = 0.001\n",
    "\n",
    "# Agregamos capas al modelo\n",
    "model.add(ConvLayer(input_shape, (kernel_size, kernel_size), kernels, lambda_reg=lambda_reg)) # Capa de Convolución con un kernels\n",
    "model.add(ActivationLayer(relu, relu_prime))\n",
    "model.add(FlattenLayer())  # Capa de Flatten para aplanar la salida\n",
    "model.add(FCLayer(input_shape[0]*input_shape[1]*kernels, 128, lambda_reg=lambda_reg))\n",
    "model.add(ActivationLayer(tanh, tanh_prime))\n",
    "model.add(FCLayer(128, 10, lambda_reg=lambda_reg)) \n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# Asignamos función de pérdida\n",
    "model.use(mse, mse_prime)\n",
    "\n",
    "# Entrenamos el modelo con datos de entrenamiento\n",
    "model.fit(X_train, y_train, epochs=10, learning_rate=0.01, batch_size=32) \n",
    "\n",
    "# Usamos el modelo para predecir sobre los datos de prueba (validación)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# Transformamos la salida en un vector one-hot encoded, es decir 0s y un 1. \n",
    "for i in range(len(y_hat)):\n",
    "    y_hat[i] = np.argmax(y_hat[i][0])\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))\n",
    "fin = time.time() \n",
    "duración = fin - inicio\n",
    "print('Se demoró {:.2f} minutos.'.format(duración/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8624b9fc-ab62-4dfb-b920-19fd5e44a1e1",
   "metadata": {},
   "source": [
    "Capa Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "83ff358b-2d80-46db-a801-17fc488446d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolingLayer(Layer):\n",
    "    def __init__(self, pool_size, stride):\n",
    "        self.pool_size = pool_size  \n",
    "        self.stride = stride  \n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        \"\"\"\n",
    "        Realiza la operación de max pooling sobre la entrada.\n",
    "        Genera una salida reducida en función del tamaño del pool y el stride.\n",
    "        \"\"\"\n",
    "        self.input = input_data\n",
    "        input_height, input_width, input_depth = input_data.shape\n",
    "\n",
    "        output_height = (input_height - self.pool_size[0]) // self.stride + 1\n",
    "        output_width = (input_width - self.pool_size[1]) // self.stride + 1\n",
    "        self.output = np.zeros((output_height, output_width, input_depth))\n",
    "\n",
    "        for d in range(input_depth):\n",
    "            for i in range(0, output_height):\n",
    "                for j in range(0, output_width):\n",
    "                    h_start = i * self.stride\n",
    "                    h_end = h_start + self.pool_size[0]\n",
    "                    w_start = j * self.stride\n",
    "                    w_end = w_start + self.pool_size[1]\n",
    "                    # Extracción de la ventana y selección de valor máximo\n",
    "                    self.output[i, j, d] = np.max(input_data[h_start:h_end, w_start:w_end, d])\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate=None):\n",
    "        \"\"\"\n",
    "        Calcula el error en la entrada basado en el error de salida.\n",
    "        Propaga el error hacia atrás a las posiciones donde se encontraban los máximos.\n",
    "        \"\"\"\n",
    "        input_height, input_width, input_depth = self.input.shape\n",
    "        in_error = np.zeros(self.input.shape)\n",
    "\n",
    "        for d in range(input_depth):\n",
    "            for i in range(0, self.output.shape[0]):\n",
    "                for j in range(0, self.output.shape[1]):\n",
    "                    h_start = i * self.stride\n",
    "                    h_end = h_start + self.pool_size[0]\n",
    "                    w_start = j * self.stride\n",
    "                    w_end = w_start + self.pool_size[1]\n",
    "                    # Selección del valor máximo para la propagación del error\n",
    "                    max_index = np.unravel_index(np.argmax(self.input[h_start:h_end, w_start:w_end, d]), (self.pool_size[0], self.pool_size[1]))\n",
    "                    # Propagación del error donde se encontraba el máximo\n",
    "                    in_error[h_start + max_index[0], w_start + max_index[1], d] += output_error[i, j, d]\n",
    "\n",
    "        return in_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958db5c5-b2e6-4c5f-8111-d5172fa1d379",
   "metadata": {},
   "source": [
    "Capa de AveragePooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ef5a8750-c44f-479b-bba5-bdf45376435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AveragePoolingLayer:\n",
    "    def __init__(self, pool_size, stride):\n",
    "        self.pool_size = pool_size  \n",
    "        self.stride = stride  \n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        \"\"\"\n",
    "        Realiza la operación de average pooling sobre la entrada.\n",
    "        Genera una salida reducida en función del tamaño del pool y el stride.\n",
    "        \"\"\"\n",
    "        self.input = input_data  \n",
    " \n",
    "        output_height = (input_data.shape[0] - self.pool_size[0]) // self.stride + 1\n",
    "        output_width = (input_data.shape[1] - self.pool_size[1]) // self.stride + 1\n",
    "        output_depth = input_data.shape[2]  \n",
    "        self.output = np.zeros((output_height, output_width, output_depth))\n",
    "        \n",
    "        for h in range(output_height):\n",
    "            for w in range(output_width):\n",
    "                for d in range(output_depth):\n",
    "                    h_start = h * self.stride\n",
    "                    w_start = w * self.stride\n",
    "                    # Extraemos la ventana actual del input\n",
    "                    window = input_data[h_start:h_start + self.pool_size[0], w_start:w_start + self.pool_size[1], d]\n",
    "                    # Calculamos el promedio de la ventana y lo almacenamos en la salida\n",
    "                    self.output[h, w, d] = np.mean(window)\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate=None):\n",
    "        \"\"\"\n",
    "        Calcula el error en la entrada basado en el error de salida.\n",
    "        Propaga el error hacia atrás promediando entre los elementos de la ventana.\n",
    "        \"\"\"\n",
    "        in_error = np.zeros(self.input.shape)\n",
    "\n",
    "        output_height = output_error.shape[0]\n",
    "        output_width = output_error.shape[1]\n",
    "        \n",
    "        for h in range(output_height):\n",
    "            for w in range(output_width):\n",
    "                for d in range(output_error.shape[2]):\n",
    "                    h_start = h * self.stride\n",
    "                    w_start = w * self.stride\n",
    "                    # Distribuimos el error de salida entre los elementos de la ventana\n",
    "                    for i in range(self.pool_size[0]):\n",
    "                        for j in range(self.pool_size[1]):\n",
    "                            in_error[h_start + i, w_start + j, d] += output_error[h, w, d] / (self.pool_size[0] * self.pool_size[1])\n",
    "\n",
    "        return in_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d7c063-4ada-4843-b46d-b2f52885923a",
   "metadata": {},
   "source": [
    "Con maxpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8f8d5efd-9d9c-43d2-9b2c-c2a9b64ade06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/10   error=0.003132\n",
      "epoch 2/10   error=0.002238\n",
      "epoch 3/10   error=0.001771\n",
      "epoch 4/10   error=0.001534\n",
      "epoch 5/10   error=0.001374\n",
      "epoch 6/10   error=0.001160\n",
      "epoch 7/10   error=0.000887\n",
      "epoch 8/10   error=0.000665\n",
      "epoch 9/10   error=0.000596\n",
      "epoch 10/10   error=0.000528\n",
      "MATRIZ DE CONFUSIÓN para modelo ANN\n",
      "[[ 91   0   0   1   0   4   1   0   0   1]\n",
      " [  0 104   1   0   0   3   0   0   3   2]\n",
      " [  3   1  85   5   3   1   5   0   3   3]\n",
      " [  0   0   2  75   0  13   0   0   4   2]\n",
      " [  2   0   0   0  87   2   3   1   2   4]\n",
      " [  2   0   0   7   1  79   2   0   5   1]\n",
      " [  2   0   1   0   4   3  86   0   1   1]\n",
      " [  0   1   2   1   3   1   0  73   0  10]\n",
      " [  2   1   0   5   2   9   7   1  64   6]\n",
      " [  0   0   0   2   5   5   1   7   1  79]] \n",
      "\n",
      "La exactitud de testeo del modelo ANN es: 0.823\n",
      "Se demoró 14.26 minutos.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "inicio = time.time()\n",
    "\n",
    "model = MiniBatchNetwork()\n",
    "\n",
    "kernels = 8\n",
    "kernel_size = 3\n",
    "\n",
    "input_shape = X_train[0].shape\n",
    "\n",
    "lambda_reg = 0.001\n",
    "\n",
    "model.add(ConvLayer(input_shape, (kernel_size, kernel_size), kernels, lambda_reg=lambda_reg))  \n",
    "model.add(ActivationLayer(relu, relu_prime))\n",
    "\n",
    "\n",
    "#Capa de max pooling 2x2\n",
    "pool_size = (2, 2)  \n",
    "stride = 2  \n",
    "model.add(MaxPoolingLayer(pool_size, stride))  \n",
    "\n",
    "model.add(FlattenLayer())  \n",
    "model.add(FCLayer(input_shape[0] * input_shape[1] * kernels // 4, 128, lambda_reg=lambda_reg)) \n",
    "model.add(ActivationLayer(tanh, tanh_prime))\n",
    "model.add(FCLayer(128, 10, lambda_reg=lambda_reg)) \n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "model.use(mse, mse_prime)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, learning_rate=0.01, batch_size=32) \n",
    "\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "for i in range(len(y_hat)):\n",
    "    y_hat[i] = np.argmax(y_hat[i][0])\n",
    "\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf, '\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test, y_hat)))\n",
    "fin = time.time() \n",
    "duración = fin - inicio\n",
    "print('Se demoró {:.2f} minutos.'.format(duración / 60))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8a3d1f-0d9a-41b5-9cf9-38a3f6f9f4b0",
   "metadata": {},
   "source": [
    "Con AveragePooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "48994786-d9d9-4fff-a478-63c067bd1ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/10   error=0.003026\n",
      "epoch 2/10   error=0.002382\n",
      "epoch 3/10   error=0.002117\n",
      "epoch 4/10   error=0.001819\n",
      "epoch 5/10   error=0.001396\n",
      "epoch 6/10   error=0.001196\n",
      "epoch 7/10   error=0.001018\n",
      "epoch 8/10   error=0.000859\n",
      "epoch 9/10   error=0.000694\n",
      "epoch 10/10   error=0.000587\n",
      "MATRIZ DE CONFUSIÓN para modelo ANN\n",
      "[[ 94   0   2   0   0   1   0   0   1   0]\n",
      " [  0 110   1   0   0   1   1   0   0   0]\n",
      " [  0   3  98   1   1   0   3   0   3   0]\n",
      " [  4   3   9  56   1  10   1   1   5   6]\n",
      " [  2   0   4   0  80   0   2   1   2  10]\n",
      " [  2   0   0   4   2  78   3   0   6   2]\n",
      " [  3   0   3   0   2   3  83   1   1   2]\n",
      " [  1   1   4   0   2   0   0  75   0   8]\n",
      " [  2   2   2   1   2   5   7   0  70   6]\n",
      " [  2   0   5   0   9   0   0   4   4  76]] \n",
      "\n",
      "La exactitud de testeo del modelo ANN es: 0.820\n",
      "Se demoró 14.33 minutos.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "inicio = time.time()\n",
    "\n",
    "model = MiniBatchNetwork()\n",
    "\n",
    "kernels = 8\n",
    "kernel_size = 3\n",
    "\n",
    "input_shape = X_train[0].shape\n",
    "\n",
    "lambda_reg = 0.001\n",
    "\n",
    "model.add(ConvLayer(input_shape, (kernel_size, kernel_size), kernels, lambda_reg=lambda_reg))  \n",
    "model.add(ActivationLayer(relu, relu_prime))\n",
    "\n",
    "\n",
    "#Capa de average pooling 2x2\n",
    "pool_size = (2, 2)  \n",
    "stride = 2  \n",
    "model.add(AveragePoolingLayer(pool_size, stride))  \n",
    "\n",
    "model.add(FlattenLayer())  \n",
    "model.add(FCLayer(input_shape[0] * input_shape[1] * kernels // 4, 128, lambda_reg=lambda_reg)) \n",
    "model.add(ActivationLayer(tanh, tanh_prime))\n",
    "model.add(FCLayer(128, 10, lambda_reg=lambda_reg)) \n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "model.use(mse, mse_prime)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, learning_rate=0.01, batch_size=32) \n",
    "\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "for i in range(len(y_hat)):\n",
    "    y_hat[i] = np.argmax(y_hat[i][0])\n",
    "\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf, '\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test, y_hat)))\n",
    "fin = time.time() \n",
    "duración = fin - inicio\n",
    "print('Se demoró {:.2f} minutos.'.format(duración / 60))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada1debe-28dc-4bb7-a98f-3dfdaa1c5679",
   "metadata": {},
   "source": [
    "Implementación técnicas de regularizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a580b-fadc-40fa-966a-96c64941b235",
   "metadata": {},
   "source": [
    "DropOut Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "32df231b-7ecd-4485-bf7d-93f7afb4210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DropoutLayer:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate  # Tasa de Dropout (por ejemplo, 0.5 significa 50% de desactivación)\n",
    "\n",
    "    def forward_propagation(self, input_data, training=True):\n",
    "        \"\"\"\n",
    "        Durante la propagación hacia adelante, si estamos en modo de entrenamiento,\n",
    "        se desactivan aleatoriamente las neuronas según la tasa de Dropout.\n",
    "        \"\"\"\n",
    "        self.input = input_data\n",
    "        if training:\n",
    "            self.mask = np.random.binomial(1, 1 - self.rate, size=input_data.shape)  \n",
    "            return self.input * self.mask  \n",
    "        else:\n",
    "            return self.input \n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate=None):\n",
    "        \"\"\"\n",
    "        Propaga el error de vuelta utilizando la misma máscara que se utilizó en la\n",
    "        propagación hacia adelante.\n",
    "        \"\"\"\n",
    "        return output_error * self.mask  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "bd744557-f1a2-4b64-8c14-e7117dfe71e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/10   error=0.003831\n",
      "epoch 2/10   error=0.003219\n",
      "epoch 3/10   error=0.003112\n",
      "epoch 4/10   error=0.003035\n",
      "epoch 5/10   error=0.002902\n",
      "epoch 6/10   error=0.002797\n",
      "epoch 7/10   error=0.002607\n",
      "epoch 8/10   error=0.002487\n",
      "epoch 9/10   error=0.002325\n",
      "epoch 10/10   error=0.002290\n",
      "MATRIZ DE CONFUSIÓN para modelo ANN\n",
      "[[90  0  1  1  0  1  3  1  0  1]\n",
      " [ 0 94  4  4  1  2  2  4  0  2]\n",
      " [ 3  6 37 18  5  1 25  4  4  6]\n",
      " [ 7  5  4 68  2  0  2  5  0  3]\n",
      " [ 5  3  1  4 56  2  9  7  4 10]\n",
      " [10  4  9 30 10  1 11  4  7 11]\n",
      " [ 8  3  5  5  4  0 68  1  1  3]\n",
      " [ 3  3  4  1  8  0  1 67  1  3]\n",
      " [ 6 10 12 15 15  2  9  5 10 13]\n",
      " [ 3  1  5  5 34  0  3 24  3 22]] \n",
      "\n",
      "La exactitud de testeo del modelo ANN es: 0.513\n",
      "Se demoró 14.28 minutos.\n"
     ]
    }
   ],
   "source": [
    "inicio = time.time()\n",
    "\n",
    "model = MiniBatchNetwork()\n",
    "\n",
    "kernels = 8\n",
    "kernel_size = 3\n",
    "\n",
    "input_shape = X_train[0].shape\n",
    "\n",
    "lambda_reg = 0.001\n",
    "\n",
    "model.add(ConvLayer(input_shape, (kernel_size, kernel_size), kernels, lambda_reg=lambda_reg))  \n",
    "model.add(ActivationLayer(relu, relu_prime))\n",
    "\n",
    "# Capa de average pooling 2x2\n",
    "pool_size = (2, 2)  \n",
    "stride = 2  \n",
    "model.add(AveragePoolingLayer(pool_size, stride))  \n",
    "\n",
    "model.add(FlattenLayer())\n",
    "\n",
    "#Capa de DropOut\n",
    "dropout_rate = 0.5  #Lobotomizando mi red neuronal\n",
    "model.add(DropoutLayer(dropout_rate))\n",
    "\n",
    "model.add(FCLayer(input_shape[0] * input_shape[1] * kernels // 4, 128, lambda_reg=lambda_reg)) \n",
    "model.add(ActivationLayer(tanh, tanh_prime))\n",
    "model.add(FCLayer(128, 10, lambda_reg=lambda_reg)) \n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "model.use(mse, mse_prime)\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, epochs=10, learning_rate=0.01, batch_size=32) \n",
    "\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "for i in range(len(y_hat)):\n",
    "    y_hat[i] = np.argmax(y_hat[i][0])\n",
    "\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf, '\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test, y_hat)))\n",
    "fin = time.time() \n",
    "duración = fin - inicio\n",
    "print('Se demoró {:.2f} minutos.'.format(duración / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29117ed2-d2a3-4d06-9548-84c2c53b32d6",
   "metadata": {},
   "source": [
    "Ruido a los pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "79ef4e71-c798-4cd3-aa5e-9346e9f0ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatchNetworkNoise(Network):\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate, batch_size, weight_noise_std=0.01):\n",
    "        if x_train[0].ndim == 1:\n",
    "            x_train = np.array([[x] for x in x_train])\n",
    "        \n",
    "        samples = len(x_train)\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            indices = np.arange(samples)\n",
    "            np.random.shuffle(indices)\n",
    "            x_train = x_train[indices]\n",
    "            y_train = y_train[indices]\n",
    "            \n",
    "            for start in range(0, samples, batch_size):\n",
    "                end = min(start + batch_size, samples)\n",
    "                x_batch = x_train[start:end]\n",
    "                y_batch = y_train[start:end]\n",
    "                \n",
    "                batch_err = 0\n",
    "                \n",
    "                for j in range(len(x_batch)):\n",
    "                    output = x_batch[j]\n",
    "                    for layer in self.layers:\n",
    "                        output = layer.forward_propagation(output)\n",
    "                    \n",
    "                    batch_err += self.loss(y_batch[j], output)\n",
    "                    error = self.loss_prime(y_batch[j], output)\n",
    "                    \n",
    "                    for layer in reversed(self.layers):\n",
    "                        error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "                # Inyección de ruido a los pesos\n",
    "                for layer in self.layers:\n",
    "                    if hasattr(layer, 'weights'):\n",
    "                        noise = np.random.normal(0, weight_noise_std, layer.weights.shape)\n",
    "                        layer.weights += noise\n",
    "\n",
    "                batch_err /= len(x_batch)\n",
    "                err += batch_err\n",
    "    \n",
    "            err /= (samples / batch_size)\n",
    "            err = np.mean(err)\n",
    "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "a402d557-ca1e-4ffa-a0aa-78a4d0080081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X, y = zip(*random.sample(list(zip(X_train, y_train)), 2000))\n",
    "\n",
    "X, y = np.array(X, dtype='float64'), np.array(y, dtype='float64')\n",
    "X = np.reshape(X, (X.shape[0], -1))\n",
    "\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.5, random_state=123)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=123)\n",
    "\n",
    "y_train_value = y_train  \n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = to_categorical(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad376104-987f-4527-b160-6672d1764301",
   "metadata": {},
   "source": [
    "Sin Ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "74604866-f3fd-402a-9540-7bc0c71dd233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/10   error=0.004521\n",
      "epoch 2/10   error=0.002387\n",
      "epoch 3/10   error=0.001676\n",
      "epoch 4/10   error=0.001266\n",
      "epoch 5/10   error=0.000919\n",
      "epoch 6/10   error=0.000735\n",
      "epoch 7/10   error=0.000483\n",
      "epoch 8/10   error=0.000334\n",
      "epoch 9/10   error=0.000259\n",
      "epoch 10/10   error=0.000152\n",
      "La exactitud de validación del modelo ANN es: 0.855\n",
      "La exactitud de testeo del modelo ANN es: 0.859\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "modelMiniBatch = MiniBatchNetwork()\n",
    "\n",
    "modelMiniBatch.add(FCLayer(entrada_dim, 128))\n",
    "modelMiniBatch.add(ActivationLayer(relu, relu_prime))\n",
    "modelMiniBatch.add(FCLayer(128, 64))\n",
    "modelMiniBatch.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "modelMiniBatch.add(FCLayer(64, 10)) \n",
    "modelMiniBatch.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "modelMiniBatch.use(bce, bce_prime)\n",
    "\n",
    "modelMiniBatch.fit(X_train, y_train, epochs=10, learning_rate=0.1, batch_size=16)\n",
    "\n",
    "y_val_hat = modelMiniBatch.predict(X_val)\n",
    "\n",
    "\n",
    "y_val_hat = np.array([np.argmax(output) for output in y_val_hat])  \n",
    "matriz_conf_val = confusion_matrix(np.argmax(y_val, axis=1), y_val_hat)\n",
    "\n",
    "\n",
    "print('La exactitud de validación del modelo ANN es: {:.3f}'.format(accuracy_score(np.argmax(y_val, axis=1), y_val_hat)))\n",
    "\n",
    "y_test_hat = modelMiniBatch.predict(X_test)\n",
    "\n",
    "y_test_hat = np.array([np.argmax(output) for output in y_test_hat]) \n",
    "\n",
    "matriz_conf_test = confusion_matrix(np.argmax(y_test, axis=1), y_test_hat)\n",
    "\n",
    "\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(np.argmax(y_test, axis=1), y_test_hat)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aba4af4-87cd-4719-b14c-7295006d1bbf",
   "metadata": {},
   "source": [
    "Con Ruido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "9bd14937-4973-4de0-a9ef-dec4359bba8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/10   error=0.210534\n",
      "epoch 2/10   error=0.114835\n",
      "epoch 3/10   error=0.076533\n",
      "epoch 4/10   error=0.056343\n",
      "epoch 5/10   error=0.046522\n",
      "epoch 6/10   error=0.042060\n",
      "epoch 7/10   error=0.031181\n",
      "epoch 8/10   error=0.025354\n",
      "epoch 9/10   error=0.012372\n",
      "epoch 10/10   error=0.012762\n",
      "La exactitud de validación del modelo ANN es: 0.835\n",
      "La exactitud de testeo del modelo ANN es: 0.847\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Necesitamos identificar cuántos nodos tiene nuestra entrada, y eso depende del tamaño de X.\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "# Crear instancia de MiniBatchNetwork\n",
    "modelMiniBatch = MiniBatchNetworkNoise()\n",
    "\n",
    "# Agregamos capas al modelo\n",
    "modelMiniBatch.add(FCLayer(entrada_dim, 128))\n",
    "modelMiniBatch.add(ActivationLayer(relu, relu_prime))\n",
    "modelMiniBatch.add(FCLayer(128, 64))\n",
    "modelMiniBatch.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "modelMiniBatch.add(FCLayer(64, 10)) \n",
    "modelMiniBatch.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "modelMiniBatch.use(bce, bce_prime)\n",
    "\n",
    "modelMiniBatch.fit(X_train, y_train, epochs=10, learning_rate=0.1, batch_size=16)\n",
    "\n",
    "y_val_hat = modelMiniBatch.predict(X_val)\n",
    "\n",
    "\n",
    "y_val_hat = np.array([np.argmax(output) for output in y_val_hat])  \n",
    "\n",
    "matriz_conf_val = confusion_matrix(np.argmax(y_val, axis=1), y_val_hat)\n",
    "\n",
    "\n",
    "print('La exactitud de validación del modelo ANN es: {:.3f}'.format(accuracy_score(np.argmax(y_val, axis=1), y_val_hat)))\n",
    "\n",
    "y_test_hat = modelMiniBatch.predict(X_test)\n",
    "\n",
    "y_test_hat = np.array([np.argmax(output) for output in y_test_hat])  # Obtenemos el índice del nodo con mayor activación\n",
    "\n",
    "matriz_conf_test = confusion_matrix(np.argmax(y_test, axis=1), y_test_hat)\n",
    "\n",
    "\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(np.argmax(y_test, axis=1), y_test_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a831af52-e134-41fd-bb2e-bf017dfe83dc",
   "metadata": {},
   "source": [
    "Comparación con DropOut Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "0743cc32-ba1e-4a30-a1cf-17412edcfeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/10   error=0.003938\n",
      "epoch 2/10   error=0.002183\n",
      "epoch 3/10   error=0.001462\n",
      "epoch 4/10   error=0.001055\n",
      "epoch 5/10   error=0.000757\n",
      "epoch 6/10   error=0.000527\n",
      "epoch 7/10   error=0.000321\n",
      "epoch 8/10   error=0.000188\n",
      "epoch 9/10   error=0.000086\n",
      "epoch 10/10   error=0.000046\n",
      "La exactitud de validación del modelo ANN es: 0.875\n",
      "La exactitud de testeo del modelo ANN es: 0.864\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "modelMiniBatch = MiniBatchNetwork()\n",
    "\n",
    "dropout_rate = 0.5  #Lobotomizando mi red neuronal\n",
    "model.add(DropoutLayer(dropout_rate))\n",
    "\n",
    "\n",
    "modelMiniBatch.add(FCLayer(entrada_dim, 128))\n",
    "modelMiniBatch.add(ActivationLayer(relu, relu_prime))\n",
    "modelMiniBatch.add(FCLayer(128, 64))\n",
    "modelMiniBatch.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "modelMiniBatch.add(FCLayer(64, 10)) \n",
    "modelMiniBatch.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "modelMiniBatch.use(bce, bce_prime)\n",
    "\n",
    "modelMiniBatch.fit(X_train, y_train, epochs=10, learning_rate=0.1, batch_size=16)\n",
    "\n",
    "y_val_hat = modelMiniBatch.predict(X_val)\n",
    "\n",
    "\n",
    "y_val_hat = np.array([np.argmax(output) for output in y_val_hat])  \n",
    "\n",
    "matriz_conf_val = confusion_matrix(np.argmax(y_val, axis=1), y_val_hat)\n",
    "\n",
    "\n",
    "print('La exactitud de validación del modelo ANN es: {:.3f}'.format(accuracy_score(np.argmax(y_val, axis=1), y_val_hat)))\n",
    "\n",
    "y_test_hat = modelMiniBatch.predict(X_test)\n",
    "\n",
    "y_test_hat = np.array([np.argmax(output) for output in y_test_hat])  \n",
    "\n",
    "matriz_conf_test = confusion_matrix(np.argmax(y_test, axis=1), y_test_hat)\n",
    "\n",
    "\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(np.argmax(y_test, axis=1), y_test_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "364e7c49-ce03-4161-bc4d-4bef05fc6821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Una vez más cargamos y procesamos el Titanic\n",
    "file = open(\"Titanic.csv\", \"r\")\n",
    "titanic = file.read()\n",
    "file.close()\n",
    "titanic = titanic.split(\"\\n\")\n",
    "for j in range(len(titanic)):\n",
    "    titanic[j] = titanic[j].split(\",\")\n",
    "\n",
    "# El nombre tiene ',' así que necesitamos volver a unirlo.\n",
    "for elemento in titanic[1:]:\n",
    "    elemento[3] = elemento[3]+elemento[4]\n",
    "    del elemento[4]\n",
    "titanic = np.array(titanic)\n",
    "titanic = np.delete(titanic, [3,8,10,11], axis=1)\n",
    "\n",
    "# Codificamos género como 0-1\n",
    "for linea in titanic[1:]:\n",
    "    linea[3] = '0' if linea[3] == 'male' else '1'\n",
    "\n",
    "# Quitamos las observaciones con datos perdidos\n",
    "borrar = []\n",
    "for l in range(len(titanic)):\n",
    "    if '' in titanic[l]:\n",
    "        borrar.append(l)\n",
    "titanic = np.delete(titanic, borrar, axis=0)\n",
    "\n",
    "# Arreglamos vector y matriz como array\n",
    "y = titanic[:,1][1:]\n",
    "X = titanic[:,2:][1:]\n",
    "X, y = np.array(X, dtype='float64'), np.array(y, dtype='float64')\n",
    "\n",
    "# Aplicamos Normalización Min-Max\n",
    "X= MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# Separamos la muestra al azar, 60% para entrenar, 40% para testeo final.\n",
    "# La proporción para entrenamiento es alta dado que el Perceptrón es sensible\n",
    "# a datos \"adversos\", así que estamos tratando de reducir ese riesgo.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8350612-8659-451f-88df-b8899d96f61d",
   "metadata": {},
   "source": [
    "Ejemplo aplicando todo al titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "06b83aa2-3085-41b2-afdd-06d2705704be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/100   error=0.539130\n",
      "epoch 2/100   error=0.486969\n",
      "epoch 3/100   error=0.468688\n",
      "epoch 4/100   error=0.461149\n",
      "epoch 5/100   error=0.451651\n",
      "epoch 6/100   error=0.446106\n",
      "epoch 7/100   error=0.436333\n",
      "epoch 8/100   error=0.435332\n",
      "epoch 9/100   error=0.421551\n",
      "epoch 10/100   error=0.419598\n",
      "epoch 11/100   error=0.417719\n",
      "epoch 12/100   error=0.416918\n",
      "epoch 13/100   error=0.409313\n",
      "epoch 14/100   error=0.410835\n",
      "epoch 15/100   error=0.405645\n",
      "epoch 16/100   error=0.408080\n",
      "epoch 17/100   error=0.400601\n",
      "epoch 18/100   error=0.399965\n",
      "epoch 19/100   error=0.396040\n",
      "epoch 20/100   error=0.395977\n",
      "epoch 21/100   error=0.394391\n",
      "epoch 22/100   error=0.391109\n",
      "epoch 23/100   error=0.395930\n",
      "epoch 24/100   error=0.386268\n",
      "epoch 25/100   error=0.386604\n",
      "epoch 26/100   error=0.381939\n",
      "epoch 27/100   error=0.385312\n",
      "epoch 28/100   error=0.385684\n",
      "epoch 29/100   error=0.383177\n",
      "epoch 30/100   error=0.389138\n",
      "epoch 31/100   error=0.382108\n",
      "epoch 32/100   error=0.378473\n",
      "epoch 33/100   error=0.380378\n",
      "epoch 34/100   error=0.381268\n",
      "epoch 35/100   error=0.380662\n",
      "epoch 36/100   error=0.378681\n",
      "epoch 37/100   error=0.382763\n",
      "epoch 38/100   error=0.378306\n",
      "epoch 39/100   error=0.375246\n",
      "epoch 40/100   error=0.377049\n",
      "epoch 41/100   error=0.369695\n",
      "epoch 42/100   error=0.378424\n",
      "epoch 43/100   error=0.369489\n",
      "epoch 44/100   error=0.369402\n",
      "epoch 45/100   error=0.370333\n",
      "epoch 46/100   error=0.376798\n",
      "epoch 47/100   error=0.374142\n",
      "epoch 48/100   error=0.369515\n",
      "epoch 49/100   error=0.368907\n",
      "epoch 50/100   error=0.367701\n",
      "epoch 51/100   error=0.364203\n",
      "epoch 52/100   error=0.366444\n",
      "epoch 53/100   error=0.368807\n",
      "epoch 54/100   error=0.366173\n",
      "epoch 55/100   error=0.368534\n",
      "epoch 56/100   error=0.365159\n",
      "epoch 57/100   error=0.366608\n",
      "epoch 58/100   error=0.371290\n",
      "epoch 59/100   error=0.369768\n",
      "epoch 60/100   error=0.363471\n",
      "epoch 61/100   error=0.362702\n",
      "epoch 62/100   error=0.366806\n",
      "epoch 63/100   error=0.367999\n",
      "epoch 64/100   error=0.367816\n",
      "epoch 65/100   error=0.362164\n",
      "epoch 66/100   error=0.370322\n",
      "epoch 67/100   error=0.361670\n",
      "epoch 68/100   error=0.360525\n",
      "epoch 69/100   error=0.357946\n",
      "epoch 70/100   error=0.363732\n",
      "epoch 71/100   error=0.351773\n",
      "epoch 72/100   error=0.361771\n",
      "epoch 73/100   error=0.356646\n",
      "epoch 74/100   error=0.353627\n",
      "epoch 75/100   error=0.357707\n",
      "epoch 76/100   error=0.355569\n",
      "epoch 77/100   error=0.356959\n",
      "epoch 78/100   error=0.359722\n",
      "epoch 79/100   error=0.363870\n",
      "epoch 80/100   error=0.353781\n",
      "epoch 81/100   error=0.359123\n",
      "epoch 82/100   error=0.354152\n",
      "epoch 83/100   error=0.351161\n",
      "epoch 84/100   error=0.352837\n",
      "epoch 85/100   error=0.355946\n",
      "epoch 86/100   error=0.353451\n",
      "epoch 87/100   error=0.352439\n",
      "epoch 88/100   error=0.353351\n",
      "epoch 89/100   error=0.354563\n",
      "epoch 90/100   error=0.366095\n",
      "epoch 91/100   error=0.353469\n",
      "epoch 92/100   error=0.349825\n",
      "epoch 93/100   error=0.348071\n",
      "epoch 94/100   error=0.347931\n",
      "epoch 95/100   error=0.351075\n",
      "epoch 96/100   error=0.354030\n",
      "epoch 97/100   error=0.357352\n",
      "epoch 98/100   error=0.352391\n",
      "epoch 99/100   error=0.348062\n",
      "epoch 100/100   error=0.344515\n",
      "MATRIZ DE CONFUSIÓN para modelo ANN\n",
      "[[146  28]\n",
      " [ 34  78]] \n",
      "\n",
      "La exactitud de testeo del modelo ANN es: 0.783\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import time\n",
    "\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "modelMiniBatch = Network()\n",
    "\n",
    "modelMiniBatch.add(FCLayer(entrada_dim, 128))\n",
    "modelMiniBatch.add(ActivationLayer(relu, relu_prime))\n",
    "\n",
    "modelMiniBatch.add(FCLayer(128, 64))\n",
    "modelMiniBatch.add(ActivationLayer(relu, relu_prime))\n",
    "\n",
    "modelMiniBatch.add(FCLayer(64, 1))\n",
    "modelMiniBatch.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "modelMiniBatch.use(bce, bce_prime)\n",
    "\n",
    "modelMiniBatch.fit(X_train, y_train, epochs=100, learning_rate=0.1)\n",
    "\n",
    "y_hat = modelMiniBatch.predict(X_test)\n",
    "\n",
    "y_hat = np.array([1 if output >= 0.5 else 0 for output in y_hat])\n",
    "\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "accuracy = accuracy_score(y_test, y_hat)\n",
    "\n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf, '\\n')\n",
    "print(f'La exactitud de testeo del modelo ANN es: {accuracy:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "78ceceff-e94f-4665-a3a3-3b5882775cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/100   error=0.021046\n",
      "epoch 2/100   error=0.018957\n",
      "epoch 3/100   error=0.018826\n",
      "epoch 4/100   error=0.017646\n",
      "epoch 5/100   error=0.018024\n",
      "epoch 6/100   error=0.017376\n",
      "epoch 7/100   error=0.017389\n",
      "epoch 8/100   error=0.017160\n",
      "epoch 9/100   error=0.016621\n",
      "epoch 10/100   error=0.016926\n",
      "epoch 11/100   error=0.016804\n",
      "epoch 12/100   error=0.016831\n",
      "epoch 13/100   error=0.016465\n",
      "epoch 14/100   error=0.016228\n",
      "epoch 15/100   error=0.016230\n",
      "epoch 16/100   error=0.016069\n",
      "epoch 17/100   error=0.015620\n",
      "epoch 18/100   error=0.015906\n",
      "epoch 19/100   error=0.016378\n",
      "epoch 20/100   error=0.015868\n",
      "epoch 21/100   error=0.015535\n",
      "epoch 22/100   error=0.015811\n",
      "epoch 23/100   error=0.015943\n",
      "epoch 24/100   error=0.015813\n",
      "epoch 25/100   error=0.015497\n",
      "epoch 26/100   error=0.015861\n",
      "epoch 27/100   error=0.015684\n",
      "epoch 28/100   error=0.015260\n",
      "epoch 29/100   error=0.015491\n",
      "epoch 30/100   error=0.015248\n",
      "epoch 31/100   error=0.014870\n",
      "epoch 32/100   error=0.015599\n",
      "epoch 33/100   error=0.015265\n",
      "epoch 34/100   error=0.015438\n",
      "epoch 35/100   error=0.015292\n",
      "epoch 36/100   error=0.015153\n",
      "epoch 37/100   error=0.015595\n",
      "epoch 38/100   error=0.014580\n",
      "epoch 39/100   error=0.014687\n",
      "epoch 40/100   error=0.015186\n",
      "epoch 41/100   error=0.014747\n",
      "epoch 42/100   error=0.014002\n",
      "epoch 43/100   error=0.014919\n",
      "epoch 44/100   error=0.014597\n",
      "epoch 45/100   error=0.014331\n",
      "epoch 46/100   error=0.014887\n",
      "epoch 47/100   error=0.014543\n",
      "epoch 48/100   error=0.014777\n",
      "epoch 49/100   error=0.014216\n",
      "epoch 50/100   error=0.014539\n",
      "epoch 51/100   error=0.014692\n",
      "epoch 52/100   error=0.014662\n",
      "epoch 53/100   error=0.014184\n",
      "epoch 54/100   error=0.014737\n",
      "epoch 55/100   error=0.014894\n",
      "epoch 56/100   error=0.014425\n",
      "epoch 57/100   error=0.014906\n",
      "epoch 58/100   error=0.014530\n",
      "epoch 59/100   error=0.014809\n",
      "epoch 60/100   error=0.014563\n",
      "epoch 61/100   error=0.014334\n",
      "epoch 62/100   error=0.014128\n",
      "epoch 63/100   error=0.014594\n",
      "epoch 64/100   error=0.014288\n",
      "epoch 65/100   error=0.014258\n",
      "epoch 66/100   error=0.014145\n",
      "epoch 67/100   error=0.014172\n",
      "epoch 68/100   error=0.014709\n",
      "epoch 69/100   error=0.014308\n",
      "epoch 70/100   error=0.014029\n",
      "epoch 71/100   error=0.014613\n",
      "epoch 72/100   error=0.013814\n",
      "epoch 73/100   error=0.014220\n",
      "epoch 74/100   error=0.014644\n",
      "epoch 75/100   error=0.014247\n",
      "epoch 76/100   error=0.014300\n",
      "epoch 77/100   error=0.014326\n",
      "epoch 78/100   error=0.014192\n",
      "epoch 79/100   error=0.014126\n",
      "epoch 80/100   error=0.013703\n",
      "epoch 81/100   error=0.014194\n",
      "epoch 82/100   error=0.013900\n",
      "epoch 83/100   error=0.013671\n",
      "epoch 84/100   error=0.014082\n",
      "epoch 85/100   error=0.013928\n",
      "epoch 86/100   error=0.014349\n",
      "epoch 87/100   error=0.014481\n",
      "epoch 88/100   error=0.013773\n",
      "epoch 89/100   error=0.013700\n",
      "epoch 90/100   error=0.013715\n",
      "epoch 91/100   error=0.013526\n",
      "epoch 92/100   error=0.013652\n",
      "epoch 93/100   error=0.013982\n",
      "epoch 94/100   error=0.013619\n",
      "epoch 95/100   error=0.013563\n",
      "epoch 96/100   error=0.013438\n",
      "epoch 97/100   error=0.013381\n",
      "epoch 98/100   error=0.014059\n",
      "epoch 99/100   error=0.014724\n",
      "epoch 100/100   error=0.013737\n",
      "MATRIZ DE CONFUSIÓN para modelo ANN\n",
      "[[141  33]\n",
      " [ 27  85]] \n",
      "\n",
      "La exactitud de testeo del modelo ANN es: 0.790\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import time\n",
    "\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "modelMiniBatch = MiniBatchNetwork()\n",
    "\n",
    "dropout_rate = 0.2  #Lobotomizando mi red neuronal\n",
    "model.add(DropoutLayer(dropout_rate))\n",
    "\n",
    "\n",
    "modelMiniBatch.add(FCLayer(entrada_dim, 128))\n",
    "modelMiniBatch.add(ActivationLayer(relu, relu_prime))\n",
    "\n",
    "modelMiniBatch.add(FCLayer(128, 64))\n",
    "modelMiniBatch.add(ActivationLayer(relu, relu_prime))\n",
    "\n",
    "modelMiniBatch.add(FCLayer(64, 1))\n",
    "modelMiniBatch.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "modelMiniBatch.use(bce, bce_prime)\n",
    "\n",
    "modelMiniBatch.fit(X_train, y_train, epochs=100, learning_rate=0.1, batch_size=16)\n",
    "\n",
    "y_hat = modelMiniBatch.predict(X_test)\n",
    "\n",
    "y_hat = np.array([1 if output >= 0.5 else 0 for output in y_hat])\n",
    "\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "accuracy = accuracy_score(y_test, y_hat)\n",
    "\n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf, '\\n')\n",
    "print(f'La exactitud de testeo del modelo ANN es: {accuracy:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c360717-90ee-40e8-bb8e-8ce530c618be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
